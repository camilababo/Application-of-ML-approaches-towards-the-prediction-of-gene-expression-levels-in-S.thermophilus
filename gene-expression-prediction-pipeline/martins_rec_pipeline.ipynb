{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting kmers from sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msbn\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mBio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SeqIO\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "#Not all are necessarily needed for the commands I included\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from Bio import SeqIO\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, cut_tree\n",
    "from scipy import stats\n",
    "import glob\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, RFECV\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pickle\n",
    "\n",
    "import shap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Function for extracting kmers from sequence and create matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "#from ch_util.sequences import extract_proteins_from_seq_record #our own package, but not needed here\n",
    "\n",
    "_complement_table = str.maketrans({\"A\": \"T\", \"T\": \"A\", \"C\": \"G\", \"G\": \"C\"})\n",
    "\n",
    "\n",
    "def extract_kmer_features(genomes, k, mode=\"nucl\", dtype=\"uint8\"):\n",
    "    \"\"\"\n",
    "    Convert genomes into a feature-table based on the presence or absence of different k-mers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    genomes: dict\n",
    "        Dictionary of genomes. Keys should be genome (strain) names, values are lists of contigs, with each contig being a SeqIO.SeqRecord\n",
    "        Alternatively, if 'mode' is set to 'prot', the values should be SeqRecords of protein sequences.\n",
    "    k: int\n",
    "        The size of the k-mers to find\n",
    "    mode: str {'nucl', 'prot', 'cds'}\n",
    "        Determines how kmers will be extracted.\n",
    "        If 'nucl', nucleotide kmers from the provided contigs will be counted.\n",
    "        If 'prot', amino acid kmers from provided protein SeqRecords will be counted.\n",
    "        If 'cds', protein sequences will be extracted from the provided contigs, and amino acid kmers will be\n",
    "        counted in these. The nucleotide SeqRecords must already be annotated with CDS features\n",
    "    dtype: str\n",
    "        The dtype used to represent the counts. Must be a valid numpy dtype. The default uint8 allows counts up to\n",
    "        255. If you expect some kmers with higher counts, select uint16 instead.\n",
    "    \"\"\"\n",
    "    if mode not in (\"nucl\", \"prot\", \"cds\"):\n",
    "        raise ValueError(f\"Invalid mode: '{mode}'. Must be 'nucl', 'prot', or 'cds'\")\n",
    "\n",
    "    n_genomes = len(genomes)\n",
    "    genome_order = list(genomes)\n",
    "\n",
    "    if mode == \"cds\":\n",
    "        genomes = {name: _proteins_from_contigs(contigs) for name, contigs in genomes.items()}\n",
    "\n",
    "    # Default function for kmer counting dictionaries\n",
    "    def new_array():\n",
    "        return np.zeros(n_genomes, dtype=dtype)\n",
    "\n",
    "    if mode == \"nucl\":\n",
    "        kmer_dict = DnaKmerDict(new_array)  # Use special reverse complementarity-aware dictionary\n",
    "    else:\n",
    "        kmer_dict = defaultdict(new_array)  # Use regular default-dictionary\n",
    "\n",
    "    # Loop over genomes and count kmers in each\n",
    "    for i, chcc in enumerate(genome_order):\n",
    "        contigs = genomes[chcc]\n",
    "        for kmer in _generate_kmers(contigs, k, mode):\n",
    "            kmer_dict[kmer][i] += 1\n",
    "\n",
    "    if mode == \"nucl\":\n",
    "        # Remove redundant kmers leaving only those that are alphabetically smaller than their reverse complement\n",
    "        kmer_dict.clear_redundant_entries()\n",
    "\n",
    "    return pd.DataFrame(kmer_dict, index=genome_order)\n",
    "\n",
    "\n",
    "def _generate_kmers(seq_records, k, mode):\n",
    "    \"\"\"\n",
    "    Extracts k-mers from an iterable of SeqRecord objects\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    seq_records: iterable\n",
    "        An iterable, e.g. list, containing SeqRecord objects\n",
    "    k: int\n",
    "        The size of the k-mers to find\n",
    "    mode: str\n",
    "        Kmer counting model. Refer to 'extract_kmer_features' documentation for description.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    kmers: generator\n",
    "        Generator of all the kmers found in the input SeqRecords\n",
    "    \"\"\"\n",
    "    if mode == \"nucl\":\n",
    "        for record in seq_records:\n",
    "            # record = str(record.seq)  # Convert to string for faster substring retrieval\n",
    "            for i in range(len(record) - k + 1):\n",
    "                kmer = record[i: i + k]\n",
    "                if \"N\" not in kmer:\n",
    "                    yield kmer\n",
    "    else:\n",
    "        for record in seq_records:\n",
    "            record = str(record.seq)  # Convert to string for faster substring retrieval\n",
    "            for i in range(len(record) - k + 1):\n",
    "                # TODO: Exclude ambiguous amino acids? Do they frequently occur?\n",
    "                yield record[i: i + k]\n",
    "\n",
    "\n",
    "def _reverse_complement_kmer(kmer):\n",
    "    return kmer.translate(_complement_table)[::-1]\n",
    "\n",
    "\n",
    "def _proteins_from_contigs(contigs):\n",
    "    \"\"\"\n",
    "    Helper function to convert a list of nucleotide SeqRecords (contigs) into a corresponding list of protein SeqRecords\n",
    "    \"\"\"\n",
    "    proteins = []\n",
    "    for contig in contigs:\n",
    "        proteins.extend(extract_proteins_from_seq_record(contig))\n",
    "    return proteins\n",
    "\n",
    "\n",
    "class DnaKmerDict(dict):\n",
    "    \"\"\"\n",
    "    Special dictionary for holding DNA kmers. Kmers that are equivalent in terms of reverse complementarity will share\n",
    "    an entry in the dictionary.\n",
    "\n",
    "    A default-function must be provided, which is used to initialize entries, similar to a collections.defaultdict.\n",
    "    Values in the kmer-dictionary should not be set explicitly, but should only be instantiated through the\n",
    "    default-function. Values should be mutable and can be modified in place. Numpy arrays are recommended for counting\n",
    "    the kmers.\n",
    "\n",
    "    Parameters:\n",
    "    default_factory: callable\n",
    "        The function that is used to make a new value in a dictionary, when a non-existing entry is retrieved\n",
    "    \"\"\"\n",
    "    def __init__(self, default_factory):\n",
    "        self.default_factory = default_factory\n",
    "\n",
    "    @staticmethod\n",
    "    def _reverse_complement(kmer):\n",
    "        \"\"\"\n",
    "        Generate the reverse complement of a nucleotide string\n",
    "        \"\"\"\n",
    "        return kmer.translate(_complement_table)[::-1]\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        \"\"\"\n",
    "        Implements the special behaviour when accessing a key that does not exist.\n",
    "        If the key is alphabetically larger than its reverse complement, the key will map to the reverse complement's\n",
    "        value\n",
    "        \"\"\"\n",
    "        rev_comp = self._reverse_complement(key)\n",
    "        if key <= rev_comp:\n",
    "            val = self[key] = self.default_factory()\n",
    "        else:\n",
    "            val = self[key] = self[rev_comp]\n",
    "        return val\n",
    "\n",
    "    def clear_redundant_entries(self):\n",
    "        \"\"\"\n",
    "        Remove all redundant keys, i.e. all the kmers that are alphabetically larger than their reverse complement.\n",
    "        \"\"\"\n",
    "        for key in list(self.keys()):\n",
    "            if key > self._reverse_complement(key):\n",
    "                del self[key]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hierarchical clustering and dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#distances\n",
    "link = linkage(matrix, metric=\"euclidean\", method=\"ward\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Plot dendrogram with phenotype data as heatmap\n",
    "fig = plt.figure(figsize=[5, 20])\n",
    "ax1 = fig.add_axes([0, 0, 0.8, 1])\n",
    "dendrogram(link, orientation=\"left\", labels=plot_df.index, leaf_font_size=1)\n",
    "leaf_order = [t.get_text() for t in ax1.get_yticklabels()]\n",
    "\n",
    "ax2 = fig.add_axes([0.82, 0, 0.1, 1])\n",
    "\n",
    "ax3 = fig.add_axes([0.01, 0.89, 0.05, 0.1])\n",
    "\n",
    "heatmap_data = phenotype_values.to_frame()\n",
    "\n",
    "heatmap_data = heatmap_data.reindex(leaf_order[::-1])\n",
    "sbn.heatmap(heatmap_data, yticklabels=True, ax=ax2, cbar_ax=ax3)\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.set_yticklabels(ax2.get_yticklabels(), rotation=0, fontsize=2)\n",
    "\n",
    "\n",
    "plt.savefig(\"dendrogram.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Machine learning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting\n",
    "x_train, x_test, y_train, y_test =  train_test_split(X, Y)\n",
    "\n",
    "#Define and fit model\n",
    "model = GradientBoostingRegressor()\n",
    "model = xgboost.XGBRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#More is possible here in terms of parameters, but you will have to consult the help and other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Compare predictions and measurements, by plot and correlation coefficient\n",
    "\n",
    "plt.figure(figsize=[10, 5])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"--\")\n",
    "plt.plot(model.predict(x_train), y_train, \".\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"--\")\n",
    "Z = model.predict(x_test)\n",
    "plt.plot(Z, y_test, \".\")\n",
    "print(stats.spearmanr(Z, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Quantity of relevant features\n",
    "#A good idea to reduce the number of features to avoid overfitting\n",
    "#When predictions do not improve by adding features, this would correspond to a good number of features to include.\n",
    "train_performances = []\n",
    "for i in range(70):\n",
    "    model_train = xgboost.XGBRegressor()\n",
    "    X_train_subset = x_train[rankings.sort_values()[:i+1].index]# [:, ranking <= i+1]\n",
    "    model_train.fit(X_train_subset, y_train)\n",
    "    z_train = model_train.predict(X_train_subset)\n",
    "    train_performances.append(stats.pearsonr(y_train, z_train)[0])\n",
    "    \n",
    "plt.plot(range(1, 71), train_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Recursive feature elimination\n",
    "\n",
    "x_train, x_test, y_train, y_test =  train_test_split(X, Y)\n",
    "\n",
    "#initial model\n",
    "model_1 = xgboost.XGBRegressor()\n",
    "model_1.fit(x_train, y_train)\n",
    "\n",
    "#identify features with very low feature importances\n",
    "#These can be removed and increase speed\n",
    "reduced_features_1 = pd.Series(model_1.feature_importances_, index=x_train.columns)\n",
    "reduced_features_1 = reduced_features_1[reduced_features_1 >= 0.001].index\n",
    "#NB, modelx.feature_importances_ is where the basic feature importances can be found. \n",
    "#They can be useful for identifying features that give high expression, and also the shap values below.\n",
    "\n",
    "#Recursive feature elimination\n",
    "rfe = RFECV(xgboost.XGBRegressor())\n",
    "\n",
    "#Train with reduced features\n",
    "rfe.fit(x_train[reduced_features_1], y_train)\n",
    "\n",
    "#Get ranking of features\n",
    "therankings = pd.Series(rfe.ranking_, index=reduced_features_1)\n",
    "#Get features with best ranking\n",
    "# [rankings[rankings <= 30]\n",
    "reduced_features_2 = therankings[therankings == 1].index\n",
    "\n",
    "#Train model on reduced features\n",
    "model_2 = xgboost.XGBRegressor()\n",
    "model_2.fit(x_train[reduced_features_2], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Save the model\n",
    "pickle.dump(model, open('models/model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Basic SHAP commands\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(model, x_test)\n",
    "shap_values = explainer.shap_values(x_test)\n",
    "\n",
    "shap.summary_plot(shap_values, x_test)\n",
    "\n",
    "shap_importance_df = global_shap_importance(modelX, x_test)\n",
    "shap_importance_df[0:20]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 0.8, 0.8])\n",
    "\n",
    "shap.summary_plot(shap_values, x_test)\n",
    "shap.summary_plot(shap_values, x_test, show=False)\n",
    "plt.savefig(\"plots/shap_summary_plot3.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#or dedicated SHAP function\n",
    "\n",
    "def shap_importances(X,\n",
    "                     trained_model,\n",
    "                     n_features=30,\n",
    "                     plot=[],\n",
    "                     n_non_genetic_features=0,\n",
    "                     tax_dict=None):\n",
    "    \"\"\"\n",
    "    ONLY for tree models!\n",
    "    Gets the most important features, as identified by Shap and makes Shap\n",
    "    feature importance plots.\n",
    "    Learn more about Shap: https://github.com/slundberg/shap\n",
    "    If the plot list contains 'summary', a Shap summary plot will be made.\n",
    "    If the plot list contains any strain IDs (from the index of X), the\n",
    "    function also makes individual Shap plots for each prediction for those\n",
    "    strain IDs. All plots are returned in a dictionary.\n",
    "\n",
    "    Input:\n",
    "    -------\n",
    "    X: Pandas DataFrame.\n",
    "        Strain IDs in index, features in the columns.\n",
    "    trained_model: Trained scikit learn tree model.\n",
    "    n_features: Integer (default=30).\n",
    "        Number of highest importance features to plot and return.\n",
    "    plot: List (default=[]).\n",
    "        Can contain the values 'summary' and IDs from X.index.\n",
    "        If the list is empty, no plots are made.\n",
    "        If it contains 'summary' it makes Shap feature importance plot.\n",
    "        If it contains IDs from X.index it makes Shap plots for each sample.\n",
    "    n_non_genetic_features: Integer (default=0).\n",
    "        Number of non-genetic features. This is used for names and labels in\n",
    "        the individual sample plots.\n",
    "    tax_dict: Dictionary.\n",
    "        Strain IDs as keys and taxonomy values. This is used for labels in\n",
    "        the individual sample plots.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    top_features: List of strings.\n",
    "        List of the highest importance features (corresponding to the plot).\n",
    "        Only returns the n_features top features.\n",
    "    figures: Dictionary.\n",
    "        Dictionary of Shap plots. Plot names as keys, plots as values.\n",
    "        The plot names are either 'summary' or they start with a strain ID\n",
    "        and contain information on the contition values of the sample.\n",
    "        Print the dictionary keys to see: print(figures.keys())\n",
    "    \"\"\"\n",
    "\n",
    "    for p in plot:\n",
    "        if p not in ['summary'] + list(X.index):\n",
    "            raise Exception('\"plot\" contains invalid values.')\n",
    "\n",
    "    # Explain the model's predictions with SHAP values:\n",
    "    explainer = shap.TreeExplainer(trained_model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    figures = {}\n",
    "\n",
    "    if 'summary' in plot:\n",
    "        fig = plt.figure()\n",
    "        # Plot the effects of the features:\n",
    "        shap.summary_plot(shap_values, X, max_display=n_features, show=False)\n",
    "        figures['summary'] = fig\n",
    "\n",
    "    # Get numeric index of which rows (samples) of X to plot:\n",
    "    rows = [i for i in range(len(X)) if X.index[i] in plot]\n",
    "\n",
    "    # Plot the effects of the features for each individual prediction:\n",
    "    for i in rows:\n",
    "        fig = shap.force_plot(explainer.expected_value,\n",
    "                              shap_values[i, :],\n",
    "                              X.iloc[i, :],\n",
    "                              matplotlib=True,\n",
    "                              show=False)  # If show=True fig is empty\n",
    "        if n_non_genetic_features > 0:\n",
    "            # Put label on x-axis to know which sample it is:\n",
    "            # X.index[i] is the strain ID\n",
    "            # tax_dict[X.index[i]] is the taxonomy\n",
    "            # X.columns is the feature list\n",
    "            plt.xlabel(X.index[i] + ', ' + tax_dict[X.index[i]] + ', ' +\n",
    "                       ', '.join([X.columns[j] + '=' + str(X.iloc[i, j])\n",
    "                                  for j in range(n_non_genetic_features)]))\n",
    "            name = X.index[i] + '__'\n",
    "            name += '__'.join([X.columns[j] + '_' + str(X.iloc[i, j])\n",
    "                               for j in range(n_non_genetic_features)])\n",
    "            figures[name] = fig\n",
    "        else:\n",
    "            # Put label on x-axis to know which sample it is:\n",
    "            plt.xlabel(X.index[i] + ', ' + tax_dict[X.index[i]])\n",
    "            figures[X.index[i]] = fig\n",
    "\n",
    "    top_features = X.columns[np.argsort(np.abs(shap_values).mean(0))[::-1]]\n",
    "    top_features = list(top_features)[:n_features]\n",
    "\n",
    "    return top_features, figures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}